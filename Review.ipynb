{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9\n",
    "\n",
    "$Bias(\\hat f) = E[\\hat f] - f$  \n",
    "Bias is how much the average of all your models (fitted on different training data) differ from the true model. It is similar to accuracy. With high complexity, the average of all your models will have low bias; and vice-versa.\n",
    "\n",
    "$Var(\\hat f) = E[(\\hat f - E[\\hat f])^2]$  \n",
    "Variance is how much your model (fitted on the training data) varies from the average from all models (fitted on different training data) given a fixed complexity . With low complexity, it won't vary as much between different training samples; with high complexity, it will vary a lot between different training samples.\n",
    "\n",
    "$MSE$  \n",
    "$= E[(y - \\hat f)^2]$  \n",
    "$= E[(f + e - \\hat f)^2]$  \n",
    "$= E[(f - \\hat f + e)^2]$  \n",
    "$= E[(f - \\hat f)^2 + e^2 + 2e(f - \\hat f)]$  \n",
    "$= E[(f - \\hat f)^2] + E[e^2] + 2E[e]E[(f - \\hat f)]$  \n",
    "$= E[(f - \\hat f)^2] + \\sigma ^2 + 2(0)E[(f - \\hat f)]$  \n",
    "$= E[(f - \\hat f)^2] + \\sigma ^2$  \n",
    "$= E[(f - E[\\hat f] + E[\\hat f] - \\hat f)^2] + \\sigma ^2$  \n",
    "$= E[(f - E[\\hat f])^2] - 2E[f - E[\\hat f]]E[-E[\\hat f] + \\hat f] + E[(-E[\\hat f] + \\hat f)^2] + \\sigma ^2$  \n",
    "$= E[(f - E[\\hat f])^2] - 2(E[f] - E[\\hat f])(-E[\\hat f] + E[\\hat f]) + E[(\\hat f - E[\\hat f])^2] + \\sigma ^2$  \n",
    "$= E[(f - E[\\hat f])^2] + E[(\\hat f - E[\\hat f])^2] + \\sigma ^2$  \n",
    "$= (f - E[\\hat f])^2 + E[(\\hat f - E[\\hat f])^2] + \\sigma ^2$  \n",
    "$= Bias(\\hat f)^2 + Var(\\hat f) + \\sigma ^2$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem\n",
    "\n",
    "$P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(B | A) P(A)}{P(B)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative vs Discriminative Model\n",
    "\n",
    "Discriminative models decision boundaries between classes\n",
    "- learns $P(Y|X)$\n",
    "- generally low bias\n",
    "- e.g. NN, Logistic, SVM, Decision Trees, Random forests (high bias), KNN (bias $\\propto$ K)\n",
    "\n",
    "Generative models models distributions of individual classes\n",
    "- learns $P(Y, X)$ [or $p(Y)$, $P(X|Y)$]\n",
    "- e.g. NB (special case of LDA), GMM, HMM, VAE, Linear Discriminant Analysis (LDA), QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "- high bias, low variance\n",
    "- supervised classifier\n",
    "- generative model\n",
    "\n",
    "\n",
    "Assumes independence $P(x_1 \\ldots x_n| y_k) = \\Pi_i P(x_i| y_k)$\n",
    "\n",
    "$P(y_k|x_1 \\ldots x_n) = \\frac{P(x_1 \\ldots x_n| y_k) P(y_k)}{P(x_1 \\dots x_n)} \\propto \\Pi_i P(x_i| y_k) P(y_k)$\n",
    "\n",
    "$\\hat y_k = argmax_k \\Pi P(x_i| y_k) P(y_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear/Quadratic Discriminant Analysis\n",
    "\n",
    "https://en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
    "\n",
    "https://sebastianraschka.com/faq/docs/lda-vs-pca.html\n",
    "\n",
    "LDA equivalent to a Gaussian Naive Bayes model without independence assumption.\n",
    "\n",
    "High Level Differences\n",
    "\n",
    "- GNB : assumes covariance of X under classes C1 and C2 are different, but the off-diagonal elements are 0.\n",
    "- LDA : assumes covariance of X under classes C1 and C2 are same, and the off-diagonal elements are not equal to 0.\n",
    "- QDA : assumes covariance of X under classes C1 and C2 are different, and the off-diagonal elements are not equal to 0.\n",
    "\n",
    "https://stackoverflow.com/questions/63023314/gaussian-nb-vs-lda-in-scikit-learn\n",
    "http://www.columbia.edu/~mh2078/MachineLearningORFE/Classification1_MasterSlides.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization\n",
    "\n",
    "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "\n",
    "https://www.youtube.com/watch?v=4jRBRDbJemM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type I vs Type II\n",
    "\n",
    "Type I (alpha)\n",
    "- H0 True -> reject\n",
    "- False Positive \n",
    "\n",
    "Type II (beta)\n",
    "- H0 False (H1 True) -> accept\n",
    "- False Negative\n",
    "\n",
    "1 - beta = Power / Recall / Sensitivity / TPR\n",
    "\n",
    "1 - alpha = TNR / Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy vs Performance\n",
    "\n",
    "Accuracy \n",
    "- $Acc = \\frac{TP + TN}{P + N}$\n",
    "\n",
    "Performance\n",
    "- $BA = \\frac{TPR + TNR}{2}$\n",
    "- $F_1 = \\frac{2PPV \\cdot TPR}{PPV + TPR}$\n",
    "\n",
    "\n",
    "Accuracy is a poor measure for class inbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging vs Boosting\n",
    "\n",
    "https://www.kaggle.com/prashant111/bagging-vs-boosting\n",
    "\n",
    "https://medium.com/syncedreview/infiniteboosting-a-bagging-boosting-hybrid-algorithm-8b109019e480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests vs Bagged Trees\n",
    "\n",
    "https://stats.stackexchange.com/questions/264129/what-is-the-difference-between-bagging-and-random-forest-if-only-one-explanatory\n",
    "\n",
    "https://www.youtube.com/watch?v=7VeUPuFGJHk\n",
    "\n",
    "https://www.youtube.com/watch?v=J4Wdy0Wc_xQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE vs MAP\n",
    "\n",
    "https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/\n",
    "\n",
    "---\n",
    "\n",
    "Recall Bayes' Theorem\n",
    "\n",
    "$\\begin{align}\n",
    "P(\\theta \\vert X) &= \\frac{P(X \\vert \\theta) P(\\theta)}{P(X)} \\\\[10pt]\n",
    "                  &\\propto P(X \\vert \\theta) P(\\theta)\n",
    "\\end{align}$\n",
    "\n",
    "where:\n",
    "  - $P(\\theta \\vert X)$ is the posterior\n",
    "  - $P(X \\vert \\theta)$ is the likelihood\n",
    "  - $P(\\theta)$ is the prior\n",
    "  - $P(X)$ is the normalizing constant\n",
    "\n",
    "---\n",
    "\n",
    "MLE is a frequentist approach as it ignores the prior\n",
    "\n",
    "$\\begin{align}\n",
    "\\theta_{MLE} &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} P(X \\vert \\theta) \\\\[10pt]\n",
    "             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log P(X \\vert \\theta) \\\\[10pt]\n",
    "             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log \\prod_i P(x_i \\vert \\theta) \\\\[10pt]\n",
    "             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\sum_i \\log P(x_i \\vert \\theta)\n",
    "\\end{align}$\n",
    "\n",
    "---\n",
    "\n",
    "Maximum A Posteriori is a Bayesian approach as it considers the Prior\n",
    "\n",
    "$\\begin{align}\n",
    "\\theta_{MAP} &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} P(X \\vert \\theta) P(\\theta) \\\\[10pt]\n",
    "             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log P(X \\vert \\theta) + \\log P(\\theta) \\\\[10pt]\n",
    "             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\log \\prod_i P(x_i \\vert \\theta) + \\log P(\\theta) \\\\[10pt]\n",
    "             &= \\mathop{\\rm arg\\,max}\\limits_{\\theta} \\sum_i \\log P(x_i \\vert \\theta) + \\log P(\\theta)\n",
    "\\end{align}$\n",
    "\n",
    "---\n",
    "\n",
    "When $P(\\theta)$ is uniform, $\\theta_{MAP} = \\theta_{MLE}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
